<h1>NEW OPTIMIZATION ALGORITHMS FOR NEURAL NETWORK TRAINING USING OPERATOR SPLITTING TECHNIQUES</h1>

<h2>Description</h2>

<p>This is the GitHub repository for the article <a href="https://arxiv.org/"> NEW OPTIMIZATION ALGORITHMS FOR NEURAL NETWORK TRAINING USING OPERATOR SPLITTING TECHNIQUES</a></p>

<p><img src="img/fig.png" alt="picture" /></p>

<h2>Abstract</h2>

<p>In the following paper we present a new type of optimization algorithms adapted for neural network training. These algorithms are based upon sequential operator splitting technique for some associated dynamical systems. Furthermore, we investigate through numerical simulations the empirical rate of convergence of these iterative schemes toward a local minimum of the loss function, with some suitable choices of the underlying hyper-parameters. We validate the convergence of these optimizers using the results of the accuracy and of the loss function on the MNIST, MNIST-Fashion and CIFAR 10 classification datasets.</p>

<h2>Requirements</h2>

<ul>
<li>pytorch</li>
<li>numpy</li>
<li>matplotlib</li>
<li>seaborn</li>
<li>dill</li>
</ul>


<p>pip install -r requirements.txt</p>

<h2>Usage</h2>

<p><code>bash
python main.py --stoch --optim SSA1 --lr 0.1
</code>
runs ssa1 on the MNIST dataset, computing the gradient and taking a step for each mini batch
<code>bash
pthon main.py --fash --stoch --epochs 5 --lr 0.5 --k 1 --optim SSA2
</code>
runs ssa2 on the MNIST Fashion Dataset for 5 epochs</p>

<h2>Help</h2>

<p><code>bash
python main.py --help
</code>
Output:
```
usage: main.py [-h] [&ndash;batch-size N] [&ndash;test-batch-size N] [&ndash;epochs N]
               [&ndash;lr LR] [&ndash;momentum M] [&ndash;q Q] [&ndash;k K] [&ndash;alpha A] [&ndash;eps E]
               [&ndash;optim O] [&ndash;loss L] [&ndash;no-cuda] [&ndash;seed S]
               [&ndash;log-interval N] [&ndash;stoch] [&ndash;cifar10] [&ndash;fash] [&ndash;optimhelp]
               [&ndash;losshelp]</p>

<p>PyTorch MNIST Example</p>

<p>optional arguments:
  -h, &ndash;help           show this help message and exit
  &ndash;batch-size N       input batch size for training (default: 64)
  &ndash;test-batch-size N  input batch size for testing (default: 1000)
  &ndash;epochs N           number of epochs to train (default: 30)
  &ndash;lr LR              learning rate (default: 0.00001)
  &ndash;momentum M         SGD momentum (default: 0.0)
  &ndash;q Q                q parameter for SSA1 algorithm (default: 2)
  &ndash;k K                k parameter for SSA1/SSA2 algorithm (default: 2)
  &ndash;alpha A            alpha parameter for the RMS running average (default:
                       0.99)
  &ndash;eps E              eps parameter for the RMS division by 0 correction
                       (default: 1e-8)
  &ndash;optim O            Optimiser to use (default: SGD)
  &ndash;loss L             Loss function (default: nll for MNIST, cross-entropy
                       for cifar10)
  &ndash;no-cuda            disables CUDA training
  &ndash;seed S             random seed (default: 1)
  &ndash;log-interval N     how many batches to wait before logging training status
  &ndash;stoch              use stochastic gradient computation
  &ndash;cifar10            Use Cifar10 not MNIST
  &ndash;fash               Use MNIST fashion not MNIST
  &ndash;optimhelp          Print optim options
  &ndash;losshelp           Print loss options
<code>
</code>bash
python main.py &ndash;optimhelp
<code>
Output:
</code>
ASGD
Adadelta
Adagrad
Adam
Adamax
LBFGS
Optimizer
RMSprop
Rprop
SGD
SSA1
SSA1Ada
SSA2
SSA2Ada
SparseAdam
<code>
</code>bash
python main.py &ndash;losshelp
<code>
Output:
</code>
AdaptiveLogSoftmaxWithLoss
BCELoss
BCEWithLogitsLoss
CTCLoss
CosineEmbeddingLoss
CrossEntropyLoss
HingeEmbeddingLoss
KLDivLoss
L1Loss
MSELoss
MarginRankingLoss
MultiLabelMarginLoss
MultiLabelSoftMarginLoss
MultiMarginLoss
NLLLoss
NLLLoss2d
PoissonNLLLoss
SmoothL1Loss
SoftMarginLoss
TripletMarginLoss
```</p>

<h2>View Saved Results</h2>

<p>```python
import os
import torch
import dill
import models</p>

<p>for root, dirs, files  in os.walk(&lsquo;results&rsquo;):
    for <em>f in files:
        with open(os.path.join(root, </em>f), &lsquo;rb&rsquo;) as f:
            data = dill.load(f)
            #do stuff
```</p>
